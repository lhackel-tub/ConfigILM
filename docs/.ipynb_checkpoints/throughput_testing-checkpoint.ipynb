{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Throughput testing\n",
    "\n",
    "During development of new models it is often useful to test the throughput of the data pipeline. To enable this, ConfigILM provides a ThroughputTest_DataModule and a corresponding ThroughputTestDataset. These datasets don't load any actual data, but instead generate a single dummy sample during initialization and return it for each call to `__getitem__()`. The fake length of the dataset can be set with the num_samples parameter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380fa46935d60943"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run the throughput test we first create the model and then pass the respective DataModule to the trainer. For more details on creating the model see [the page on VQA model creation](vqa.ipynb). The code here is almost identical with some reduced parts."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d15ddafc59490f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4942698902f49",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from configilm import ConfigILM\n",
    "\n",
    "class LitVQAEncoder(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Wrapper around a pytorch module, allowing this module to be used in automatic\n",
    "    training with pytorch lightning.\n",
    "    Among other things, the wrapper allows us to do automatic training and removes the\n",
    "    need to manage data on different devices (e.g. GPU and CPU).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ConfigILM.ILMConfiguration,\n",
    "        lr: float = 1e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.config = config\n",
    "        self.model = ConfigILM.ConfigILM(config)\n",
    "        self.val_output_list = []\n",
    "        self.test_output_list = []\n",
    "\n",
    "    def _disassemble_batch(self, batch):\n",
    "        images, questions, labels = batch\n",
    "        # transposing tensor, needed for Huggingface-Dataloader combination\n",
    "        questions = torch.tensor(\n",
    "            [x.tolist() for x in questions], device=self.device\n",
    "        ).T.int()\n",
    "        return (images, questions), labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = self._disassemble_batch(batch)\n",
    "        x_hat = self.model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(x_hat, y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        return optimizer\n",
    "\n",
    "    # ============== NON-MANDATORY-FUNCTION ===============\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self._disassemble_batch(batch)\n",
    "        x_hat = self.model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(x_hat, y)\n",
    "        self.val_output_list += [{\"loss\": loss, \"outputs\": x_hat, \"labels\": y}]\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        super().on_validation_epoch_start()\n",
    "        self.val_output_list = []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in self.val_output_list]).mean()\n",
    "        self.log(\"val/loss\", avg_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = self._disassemble_batch(batch)\n",
    "        x_hat = self.model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(x_hat, y)\n",
    "        self.test_output_list += [{\"loss\": loss, \"outputs\": x_hat, \"labels\": y}]\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        super().on_test_epoch_start()\n",
    "        self.test_output_list = []\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in self.test_output_list]).mean()\n",
    "        self.log(\"test/loss\", avg_loss)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # because we are a wrapper, we call the inner function manually\n",
    "        return self.model(batch)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=4,\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=1,\n",
    "    logger=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from configilm.ConfigILM import ILMConfiguration, ILMType\n",
    "image_model_name = \"resnet18\"\n",
    "text_model_name = \"prajjwal1/bert-tiny\"\n",
    "number_of_channels = 12\n",
    "image_size = 120\n",
    "lr = 5e-4\n",
    "seq_len = 32\n",
    "classes = 25\n",
    "\n",
    "model_config = ILMConfiguration(\n",
    "    timm_model_name=image_model_name,\n",
    "    hf_model_name=text_model_name,\n",
    "    classes=classes,\n",
    "    image_size=image_size,\n",
    "    channels=number_of_channels,\n",
    "    network_type=ILMType.VQA_CLASSIFICATION,\n",
    "    max_sequence_length=seq_len,\n",
    ")\n",
    "model = LitVQAEncoder(config=model_config, lr=lr)"
   ],
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "id": "initial_id",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the model is trained using the `ThroughputTestDataModule` instead of any real data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "623f5d4dab8fd2f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from configilm.extra.DataModules import ThroughputTest_DataModule\n",
    "dm = ThroughputTest_DataModule.ThroughputTestDataModule(\n",
    "    data_dirs={},  # parameter is ignored but required for compatibility with other DataModules in ConfigILM\n",
    "    img_size=(number_of_channels, image_size, image_size),\n",
    "    seq_length=seq_len,\n",
    "    num_samples=32*16,  # number of \"samples\" in this dataset -> each sample is the same one\n",
    "    batch_size=32,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4191110b884ecb82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train the model using this fake DataModule."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e2c0fd7574cffd3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba01b8185dc4e0d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "and we can also evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656667f471c10bea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.test(model, dm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "978b8ce28ce65e48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
